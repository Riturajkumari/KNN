{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTqr7KjqzZasn0p049unWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/KNN/blob/main/KNN_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is the KNN algorithm?"
      ],
      "metadata": {
        "id": "j3sO5fGBqgOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
        "- K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
        "- K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
        "- K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
        "- K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
        "- It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
        "- KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data."
      ],
      "metadata": {
        "id": "U9dmXoI9LmFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. How do you choose the value of K in KNN?**"
      ],
      "metadata": {
        "id": "UmacNW57qjxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-NN working can be explained on the basis of the below algorithm:\n",
        "\n",
        "- Step-1: Select the number K of the neighbors\n",
        "- Step-2: Calculate the Euclidean distance of K number of neighbors\n",
        "- Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
        "- Step-4: Among these k neighbors, count the number of the data points in each category.\n",
        "- Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
        "- Step-6: Our model is ready."
      ],
      "metadata": {
        "id": "lRYOQszGNOfP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1etVquRqrGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the difference between KNN classifier and KNN regressor?**"
      ],
      "metadata": {
        "id": "Fsd5LXj2qrVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KNN can be used for regression and classification problems. When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.\n",
        "-  When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances."
      ],
      "metadata": {
        "id": "BLN-bbYAOBfx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ixa3PgRqqzil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. How do you measure the performance of KNN?**"
      ],
      "metadata": {
        "id": "TDtWc2ImqzvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The performance of KNN can be measured using various metrics such as accuracy, precision, recall, and F1 score. The accuracy is the ratio of the number of correct predictions to the total number of predictions made. Precision is the ratio of true positives to the total number of predicted positives. Recall is the ratio of true positives to the total number of actual positives.\n",
        "\n",
        "1.True Positives (TP) is defined by the total number of accurate outputs when the actual class of the data object was True and the prediction was also the True value.\n",
        "\n",
        "2.\n",
        "True Negatives (TN) is defined by the total number of accurate outputs when the actual class of the data object was False and the predicted is also the False value.\n",
        "\n",
        "3.\n",
        "False Positives (FP) when the actual class of the data object was False and the output value was the True value\n",
        "\n",
        "4.\n",
        "False Negatives (FN) when the actual class of the data object was True and the output value was the False value."
      ],
      "metadata": {
        "id": "nA4dxAwUOUOH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CMfKlKAq5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is the curse of dimensionality in KNN?**"
      ],
      "metadata": {
        "id": "YujbJ20sq5sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- K Nearest Neighbor (KNN) is a very simple, easy-to-understand, and versatile machine learning algorithm. It’s used in many different areas, such as handwriting detection, image recognition, and video recognition. KNN is most useful when labeled data is too expensive or impossible to obtain, and it can achieve high accuracy in a wide variety of prediction-type problems."
      ],
      "metadata": {
        "id": "kIMktM5RPvkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KNN is a simple algorithm, based on the local minimum of the target function which is used to learn an unknown function of desired precision and accuracy. The algorithm also finds the neighborhood of an unknown input, its range or distance from it, and other parameters. It’s based on the principle of “information gain”—the algorithm finds out which is most suitable to predict an unknown value."
      ],
      "metadata": {
        "id": "DZtYTgMjP3oV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMzm9CAjq_h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How do you handle missing values in KNN?**"
      ],
      "metadata": {
        "id": "viNsLqLzq__c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KNN imputation is a method of handling missing values in datasets. It works by imputing the missing values using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n",
        "By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors."
      ],
      "metadata": {
        "id": "HnCVuRbFQK_E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YG85sxbrEuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5r0vJfBrFru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?**"
      ],
      "metadata": {
        "id": "ZIawfYhlrF8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KNN (K-Nearest Neighbors) is a type of instance-based learning that can be used for both classification and regression problems. The difference between KNN classifier and regressor is that the former predicts the class of a given test observation by finding the most common class among its k nearest neighbors while the latter predicts the value of a continuous target variable by taking the average of its k nearest neighbors.\n",
        "\n",
        "- In terms of performance, KNN classifier is generally faster than KNN regressor because it only needs to find the most common class among its k nearest neighbors while KNN regressor needs to calculate the mean of its k nearest neighbors. However, KNN regressor can be more accurate than KNN classifier when dealing with continuous target variables.\n",
        "\n",
        "- Therefore, if you have a classification problem, you should use KNN classifier. If you have a regression problem, you should use KNN regressor.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcExaqTMRKcY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdMTVisXrLcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?**"
      ],
      "metadata": {
        "id": "7HpgWUbxrLsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The K-Nearest Neighbors (KNN) algorithm is a simple and effective machine learning algorithm that can be used for both classification and regression tasks. It is easy to implement and has high accuracy.\n",
        "\n",
        "  However, it has some weaknesses such as being computationally expensive for large datasets, sensitive to irrelevant features, and requires careful selection of the number of neighbors."
      ],
      "metadata": {
        "id": "krwvNTGKSDRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address these weaknesses, one can use dimensionality reduction techniques such as Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) to reduce the number of features in the dataset.\n",
        "- strengths of KNN algorithm :\n",
        "\n",
        "1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\n",
        "\n",
        "2. Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.\n",
        "\n",
        "3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)"
      ],
      "metadata": {
        "id": "XVb31-86STSc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JEWzuharQ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-96SqCNrR-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**"
      ],
      "metadata": {
        "id": "wGEXDrGvrSiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distance and Manhattan distance are two ways of measuring the distance between two points. Manhattan distance is based on absolute value distance, whereas Euclidean distance is based on squared error distance.\n"
      ],
      "metadata": {
        "id": "07AV5CDbgr1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euclidean Distance**\n",
        "\n",
        "This is nothing but the cartesian distance between the two points which are in the plane/hyperplane. Euclidean distance can also be visualized as the length of the straight line that joins the two points which are into consideration. This metric helps us calculate the net displacement done between the two states of an object.\n",
        "\n",
        "d\\left ( x,y \\right )=\\sqrt{\\sum_{i=1}^{n}\\left ( x_i-y_i \\right )^2}\n",
        "\n",
        "**Manhattan Distance**\n",
        "\n",
        "This distance metric is generally used when we are interested in the total distance traveled by the object instead of the displacement. This metric is calculated by summing the absolute difference between the coordinates of the points in n-dimensions.\n",
        "\n",
        "d\\left ( x,y \\right )={\\sum_{i=1}^{n}\\left | x_i-y_i \\right |}"
      ],
      "metadata": {
        "id": "UKFu3DieiCmr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxAJ_vx1rY_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mx43-MvirZvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What is the role of feature scaling in KNN?**"
      ],
      "metadata": {
        "id": "izd8HV4qrcPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature scaling is used in KNN to ensure that all features have equal weight in distance calculations. Scaling all features to a common scale gives each feature an equal weight in distance calculations. This is crucial for the KNN algorithm as it helps in preventing features with larger magnitudes from dominating the distance calculations.\n",
        "- This is crucial for the KNN algorithm as it helps in preventing features with larger magnitudes from dominating the distance calculations."
      ],
      "metadata": {
        "id": "2-XmFKdPjlXR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZZjSVCnrdvZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}