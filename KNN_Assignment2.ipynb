{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjgG9QCOuGfOS39QdERoKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/KNN/blob/main/KNN_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
        "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?**"
      ],
      "metadata": {
        "id": "yt8k6cRfnyrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Euclidean Distance:**\n",
        "Euclidean distance is one of the most used distance metrics. It is calculated using Minkowski Distance formula by setting p’s value to 2. This will update the distance ‘d’ formula as below:\n",
        "\n",
        "d\\left ( x,y \\right )=\\sqrt{\\sum_{i=1}^{n}\\left ( x_i-y_i \\right )^2}\n",
        "\n",
        "**Euclidean distance** formula can be used to calculate the distance between two data points in a plane.\n",
        "- Euclidean distance can also be visualized as the length of the straight line that joins the two points which are into consideration. This metric helps us calculate the net displacement done between the two states of an object.\n",
        "\n"
      ],
      "metadata": {
        "id": "gEBNLfqtosEc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOTpDlGdJAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manhattan Distance**:\n",
        "\n",
        "This distance metric is generally used when we are interested in the total distance traveled by the object instead of the displacement. This metric is calculated by summing the absolute difference between the coordinates of the points in n-dimensions."
      ],
      "metadata": {
        "id": "nrxCBYkQpZqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Euclidean distance metric is more sensitive to outliers than the Manhattan distance metric. This means that if there are outliers in the data, then the Euclidean distance metric may not perform as well as the Manhattan distance metric.\n",
        "\n",
        "d\\left ( x,y \\right )={\\sum_{i=1}^{n}\\left | x_i-y_i \\right |}"
      ],
      "metadata": {
        "id": "-IadAizkpsX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**difference affect the performance of a KNN classifier or regressor**\n",
        "\n",
        "The key difference between KNN classifier and KNN regression is that KNN regression tries to predict the value of the output variable by using a local average, while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability"
      ],
      "metadata": {
        "id": "Smaf82SEq_Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " KNN Classifier is used for classification problems and KNN regression is used for solving regression problems. We do a classification when the response variable is a factor with levels, and we classify the response into levels1."
      ],
      "metadata": {
        "id": "q2-GHBPwrSvA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_CeXZ3AUMrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
        "used to determine the optimal k value?**"
      ],
      "metadata": {
        "id": "Xr-Kkaf8n1PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The optimal value of k for a KNN classifier or regressor depends on the nature of the problem. There are no pre-defined statistical methods to find the most favorable value of K. Choosing a very small value of K leads to unstable decision boundaries.\n",
        "- it is recommended to use cross-validation to choose the optimal value of K. The idea is to split the data into training and validation sets and then use the validation set to evaluate the performance of the model with different values of K."
      ],
      "metadata": {
        "id": "TLA8Rr5YUh5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The kNN algorithm can be considered a voting system, where the majority class label determines the class label of a new data point among its nearest ‘k’ (where k is an integer) neighbors in the feature space.\n",
        "- kNN algorithm works, where the majority class label determines the class label of a new data point among its k nearest neighbors."
      ],
      "metadata": {
        "id": "9KTNRmfhaXiO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-Uk7qCQn5SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
        "what situations might you choose one distance metric over the other?**"
      ],
      "metadata": {
        "id": "ETlMtsMVn5f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The choice of distance metric can significantly affect the performance of a KNN classifier or regressor. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances.\n",
        "- evaluating the performance (measured by accuracy, precision, and recall) of the KNN using a large number of distance measures, tested on a number of real-world data sets, with and without adding different levels of noise."
      ],
      "metadata": {
        "id": "GPecgXKia0Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DzMH9xYn91b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
        "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
        "model performance?**"
      ],
      "metadata": {
        "id": "UmHyDE1Ln-L4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **KNN classifiers and regressors have several hyperparameters that can be tuned to improve the performance of the model. Some of the common hyperparameters are:**\n",
        "\n",
        "- n_neighbors: The number of neighbors to consider. Increasing this value can lead to a smoother decision boundary but can also lead to overfitting.\n",
        "- weights: The weight function used in prediction. The default is ‘uniform’, which means all points in each neighborhood are weighted equally. Another option is ‘distance’, which weights points by the inverse of their distance.\n",
        "algorithm: The algorithm used to compute nearest neighbors. The default is ‘auto’, which selects the most appropriate algorithm based on the values passed to fit method.\n",
        "- leaf_size: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree.\n",
        "- p: Power parameter for the Minkowski metric. When p=1, this is equivalent to using manhattan_distance (l1), and when p=2, this is equivalent to using euclidean_distance (l2)."
      ],
      "metadata": {
        "id": "IQwl6NERb5of"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAxPsTsEJE_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tune Hyperparameters Using GridSearchCV*:\n",
        "\n",
        "Grid search cross-validation builds multiple models using different combinations of hyperparameters and combination performs the best."
      ],
      "metadata": {
        "id": "61tqyzthcUuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **evaluation metrics**:\n",
        "\n",
        "- for classification, we can use accuracy score, precision, recall, f1 score or roc-auc,\n",
        "\n",
        "- for regression, we can use MAE, RMSE or R-squared"
      ],
      "metadata": {
        "id": "PJ6IPhwJcmaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=2, noise=10, random_state=42)"
      ],
      "metadata": {
        "id": "0XO3DH-Gd3Jr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "_UpY0f2mJ6k0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "metadata": {
        "id": "t1t0zErrHqpv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor=KNeighborsRegressor(n_neighbors=6,algorithm='auto')\n",
        "regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "BxZUgM00dQLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0ccd8335-b092-469a-b2d0-788b47371bbb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor(n_neighbors=6)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(n_neighbors=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(n_neighbors=6)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "qE-G3q8ZT9Mh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error"
      ],
      "metadata": {
        "id": "nPZMHtooT9XK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test,y_pred))\n",
        "print(mean_absolute_error(y_test,y_pred))\n",
        "print(mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeO1GCfzUDTG",
        "outputId": "ec283b98-d179-4777-a09d-721b129764d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9189275159979495\n",
            "9.009462452972217\n",
            "127.45860414317289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WblzlF7OUKNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
        "techniques can be used to optimize the size of the training set?**"
      ],
      "metadata": {
        "id": "n4v7WH6XoCxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The size of the training set can affect the performance of a KNN classifier or regressor. When the size of the training set is too small, the model may not be able to capture the underlying patterns in the data and may overfit.\n",
        "\n",
        "- To optimize the size of the training set, you can use techniques such as cross-validation and grid search. Cross-validation is a technique that involves splitting the data into multiple subsets and using each subset for training and testing."
      ],
      "metadata": {
        "id": "zCbzsTzHV6KK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ReYCKrMoIPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
        "overcome these drawbacks to improve the performance of the model?**"
      ],
      "metadata": {
        "id": "w3R2mg7LoIfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Potential drawbacks of using KNN as a classifier or regressor include:*\n",
        "\n",
        "- Accuracy depends on the quality of the data.\n",
        "- With large data, the prediction stage might be slow.\n",
        "- Sensitive to the scale of the data and irrelevant features.\n",
        "- Require high memory – need to store all of the training data."
      ],
      "metadata": {
        "id": "2It3s4a0Yvjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To overcome these drawbacks and improve the performance of the model, you can:*\n",
        "\n",
        "- Normalize the data to avoid sensitivity to scale.\n",
        "- Use feature selection techniques to remove irrelevant features.\n",
        "- Use dimensionality reduction techniques to reduce the number of features."
      ],
      "metadata": {
        "id": "HIvCkV6aY5-M"
      }
    }
  ]
}